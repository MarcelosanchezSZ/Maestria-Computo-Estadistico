{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea_7final_final_de_fnales.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQkl-m4FgC0c"
      },
      "source": [
        "<center>Tópicos selectos de análisis de datos<center>\n",
        "<center>Tarea 7<center>\n",
        "<center>Para entregar el 29 de noviembre de 2021<center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f46yFbqogC0h"
      },
      "source": [
        "1. En el notebook `MT_attention.ipynb` se encuentra la implementación de un modelo de traducción automática NMT (Neural Machine Translation) basado en una arquitectura sequence to sequence con atención. El modelo que se implementa está basado en LSTM bidireccional para el encoder y LSTM simple para el decoder, y se muestra en la siguiente figura:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ_orZd8ZZXI"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(retina=True, filename='model_nmt.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUYjXS-VgC0j"
      },
      "source": [
        "a) Basándote en ése modelo, obtén un modelo NMT para inglés-español y español-inglés. El corpus paralelo se obtuvo de ManyThings.org: http://www.manythings.org/anki/. Si deseas, puedes modificar el modelo, usando por ejemplo GRU en vez de LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3MStipOhTmp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80bd3b64-e498-4752-be40-c760649a1414"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwZtphRmgoej"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yU6yGpAorrs"
      },
      "source": [
        "data_path =  '/content/drive/MyDrive/Tarea7-CDII/spa.txt'\n",
        "df = pd.read_csv(data_path,encoding='utf-8', sep='\t',  names=['English', 'Spanish', 'Attribution'])\n",
        "# drop attribution\n",
        "df.drop(['Attribution'], axis=1, inplace=True)\n",
        "# remove multiple spaces\n",
        "df.English = df.English.apply(lambda x: \" \".join(x.split()))\n",
        "df.Spanish = df.Spanish.apply(lambda x: \" \".join(x.split()))\n",
        "# lowercase\n",
        "df.English = df.English.apply(lambda x: x.lower())\n",
        "df.Spanish = df.Spanish.apply(lambda x: x.lower())\n",
        "# remove punctuations\n",
        "#translator= str.maketrans('','', string.punctuation)\n",
        "#df.English= df.English.apply(lambda x: x.translate(translator))\n",
        "#df.Spanish= df.Spanish.apply(lambda x: x.translate(translator))\n",
        "# remove digits\n",
        "df.English= df.English.apply(lambda x: re.sub(r'[\\d]+','', x))\n",
        "df.Spanish= df.Spanish.apply(lambda x: re.sub(r'[\\d]+','', x))\n",
        "\n",
        "# Add start and end tokens to sentences\n",
        "df['Spanish'] =df.Spanish.apply(lambda x: '<start> '+ x + ' <end>')\n",
        "df['English'] =df.English.apply(lambda x: '<start> '+ x + ' <end>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrQJwkqMp43B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "b291ca8e-ba17-48a8-e1c7-af753066f229"
      },
      "source": [
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Spanish</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>98201</th>\n",
              "      <td>&lt;start&gt; it wasn't as expensive as i expected. ...</td>\n",
              "      <td>&lt;start&gt; no fue tan caro como esperaba. &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20842</th>\n",
              "      <td>&lt;start&gt; the bread is fresh. &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; el pan está fresco. &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130906</th>\n",
              "      <td>&lt;start&gt; this problem is too simple, so it is h...</td>\n",
              "      <td>&lt;start&gt; este problema es demasiado sencillo, p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19425</th>\n",
              "      <td>&lt;start&gt; i have one brother. &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; tengo un hermano. &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42122</th>\n",
              "      <td>&lt;start&gt; i met her along the way. &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; me encontré con ella en el camino. &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97182</th>\n",
              "      <td>&lt;start&gt; he devoted his whole life to science. ...</td>\n",
              "      <td>&lt;start&gt; consagró toda su vida a la ciencia. &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117284</th>\n",
              "      <td>&lt;start&gt; tom is very likely to know what we sho...</td>\n",
              "      <td>&lt;start&gt; lo más probable es que tom sepa lo que...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77197</th>\n",
              "      <td>&lt;start&gt; tom's enthusiasm is infectious. &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; el entusiasmo de tom es contagioso. &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57258</th>\n",
              "      <td>&lt;start&gt; my aunt brought me flowers. &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; mi tía me trajo flores. &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85260</th>\n",
              "      <td>&lt;start&gt; tom asked me for my phone number. &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; tom me pidió mi número de teléfono. &lt;end&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  English                                            Spanish\n",
              "98201   <start> it wasn't as expensive as i expected. ...       <start> no fue tan caro como esperaba. <end>\n",
              "20842                   <start> the bread is fresh. <end>                  <start> el pan está fresco. <end>\n",
              "130906  <start> this problem is too simple, so it is h...  <start> este problema es demasiado sencillo, p...\n",
              "19425                   <start> i have one brother. <end>                    <start> tengo un hermano. <end>\n",
              "42122              <start> i met her along the way. <end>   <start> me encontré con ella en el camino. <end>\n",
              "97182   <start> he devoted his whole life to science. ...  <start> consagró toda su vida a la ciencia. <end>\n",
              "117284  <start> tom is very likely to know what we sho...  <start> lo más probable es que tom sepa lo que...\n",
              "77197       <start> tom's enthusiasm is infectious. <end>  <start> el entusiasmo de tom es contagioso. <end>\n",
              "57258           <start> my aunt brought me flowers. <end>              <start> mi tía me trajo flores. <end>\n",
              "85260     <start> tom asked me for my phone number. <end>  <start> tom me pidió mi número de teléfono. <end>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP7FSMwdp95-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7757055d-bc5c-4f9d-ebe5-86d3583d0c6f"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# convert to list for tokenizer\n",
        "source_texts = df.English.to_list()\n",
        "target_texts = df.Spanish.to_list()\n",
        "\n",
        "def tokenize_sent(text):\n",
        "  '''\n",
        "  Take list on texts as input and \n",
        "  returns its tokenizer and enocded text\n",
        "  '''\n",
        "  tokenizer = Tokenizer(filters = '')\n",
        "  tokenizer.fit_on_texts(text)\n",
        "\n",
        "  return tokenizer, tokenizer.texts_to_sequences(text)\n",
        "\n",
        "# Tokenize source and target sentences\n",
        "source_tokenizer, source_encoded= tokenize_sent(text= source_texts)\n",
        "target_tokenizer, target_encoded= tokenize_sent(text= target_texts)\n",
        "\n",
        "# diccionarios para el mapeo entre one-hot encodings y vocabulario\n",
        "source_index_word = source_tokenizer.index_word\n",
        "source_word_index= source_tokenizer.word_index\n",
        "source_VOCAB_SIZE = len(source_tokenizer.word_counts)+1\n",
        "target_index_word = target_tokenizer.index_word\n",
        "target_word_index= target_tokenizer.word_index\n",
        "target_VOCAB_SIZE=len(target_tokenizer.word_counts)+1\n",
        "\n",
        "max_source_len = 0\n",
        "for i in range(len(source_encoded)):\n",
        "  if len(source_encoded[i]) > max_source_len:\n",
        "    max_source_len= len(source_encoded[i])\n",
        "\n",
        "max_target_len = 0\n",
        "for i in range(len(target_encoded)):\n",
        "  if len(source_encoded[i]) > max_target_len:\n",
        "    max_target_len= len(target_encoded[i])\n",
        "\n",
        "print('Source language')\n",
        "print('Vocabulary size: ', source_VOCAB_SIZE, 'Max sequence length: ', max_source_len)\n",
        "print('Target language')\n",
        "print('Vocabulary size: ', target_VOCAB_SIZE, 'Max sequence length: ', max_target_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source language\n",
            "Vocabulary size:  24663 Max sequence length:  72\n",
            "Target language\n",
            "Vocabulary size:  44123 Max sequence length:  70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiMOZni4p988"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "source_padded = pad_sequences(source_encoded, maxlen=max_source_len, padding='post')\n",
        "target_padded = pad_sequences(target_encoded, maxlen=max_target_len, padding='post')\n",
        "# converting to array for processing\n",
        "source_padded= np.array(source_padded)\n",
        "target_padded= np.array(target_padded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu4lrqfwp-BC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "506845a4-344e-4ce8-eb39-1b35d4d136c2"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(source_padded, target_padded, test_size=0.1, random_state=0)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((121262, 72), (13474, 72), (121262, 70), (13474, 70))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ-9_eFoDDgN"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "#from tensorflow.python.keras.layers import Layer\n",
        "#from tensorflow.python.keras import backend as K\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W_a_dot_s.shape)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>',U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
        "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
        "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
        "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
        "            return fake_state\n",
        "\n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roIarJtTp-Ev"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Embedding, Concatenate, Dropout\n",
        "from tensorflow.keras import Input, Model\n",
        "from keras.layers.recurrent import LSTMCell\n",
        "\n",
        "def get_encoder(vocab_size, source_len):\n",
        "    encoder_inputs = Input(shape=(source_len,)) \n",
        "    enc_emb = Embedding(vocab_size, 1024)(encoder_inputs)\n",
        "    # Bidirectional lstm layer\n",
        "    enc_lstm1 = Bidirectional(LSTM(256,return_sequences=True,return_state=True))\n",
        "    encoder_outputs_lstm1, forw_state_h, forw_state_c, back_state_h, back_state_c = enc_lstm1(enc_emb)\n",
        "    final_enc_h = Concatenate()([forw_state_h,back_state_h])\n",
        "    final_enc_c = Concatenate()([forw_state_c,back_state_c])\n",
        "    # concatenate encoder states\n",
        "    encoder_states =[final_enc_h, final_enc_c]\n",
        "    \n",
        "    return encoder_inputs, encoder_outputs_lstm1, final_enc_h, final_enc_c\n",
        "\n",
        "def get_decoder(vocab_size, encoder_states):\n",
        "    decoder_inputs = Input(shape=(None,)) \n",
        "    dec_emb_layer = Embedding(vocab_size, 1024) \n",
        "    dec_emb = dec_emb_layer(decoder_inputs)\n",
        "    #LSTM using encoder_states as initial state\n",
        "    decoder_lstm = LSTM(512, return_sequences=True, return_state=True) \n",
        "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "    \n",
        "    return(decoder_inputs, decoder_outputs, dec_emb_layer, decoder_lstm)\n",
        "\n",
        "def get_model(source_vocabsize, source_maxlen, target_vocabsize):\n",
        "    # encoder\n",
        "    encoder_inputs, encoder_outputs1, final_enc_h, final_enc_c = get_encoder(source_vocabsize, source_maxlen)\n",
        "    # concatenate encoder states\n",
        "    encoder_states =[final_enc_h, final_enc_c]\n",
        "    # decoder\n",
        "    decoder_inputs, decoder_outputs, dec_emb_layer, decoder_lstm = get_decoder(target_vocabsize, encoder_states)\n",
        "    #Attention Layer\n",
        "    attention_layer = AttentionLayer()\n",
        "    attention_result, attention_weights = attention_layer([encoder_outputs1, decoder_outputs])\n",
        "    # Concat attention output and decoder LSTM output \n",
        "    decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attention_result])\n",
        "    #Dense layer\n",
        "    decoder_dense = Dense(target_vocabsize, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "    # Define the model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
        "    \n",
        "    return model, encoder_inputs, encoder_outputs1, final_enc_h, final_enc_c, dec_emb_layer, \\\n",
        "            decoder_inputs, decoder_dense, decoder_lstm, attention_layer\n",
        "\n",
        "def get_inference_model(encoder_inputs, encoder_outputs1, final_enc_h, final_enc_c, dec_emb_layer, \\\n",
        "                        decoder_inputs, decoder_dense, decoder_lstm, attention_layer):\n",
        "    encoder_model = Model(encoder_inputs, outputs = [encoder_outputs1, final_enc_h, final_enc_c])\n",
        "    decoder_state_h = Input(shape=(512,))\n",
        "    decoder_state_c = Input(shape=(512,))\n",
        "    decoder_hidden_state_input = Input(shape=(max_source_len,512))\n",
        "    dec_states = [decoder_state_h, decoder_state_c]\n",
        "    dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "    decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=dec_states)\n",
        "\n",
        "    # Attention inference\n",
        "    attention_result_inf, attention_weights_inf = attention_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "    decoder_concat_input_inf = Concatenate(axis=-1, name='concat_layer')([decoder_outputs2, attention_result_inf])\n",
        "\n",
        "    dec_states2= [state_h2, state_c2]\n",
        "    decoder_outputs2 = decoder_dense(decoder_concat_input_inf)\n",
        "\n",
        "    decoder_model= Model(\n",
        "                        [decoder_inputs] + [decoder_hidden_state_input, decoder_state_h, decoder_state_c],\n",
        "                         [decoder_outputs2]+ dec_states2 + [attention_weights_inf] + [attention_result_inf])\n",
        "    \n",
        "    return encoder_model, decoder_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx9jbp8ZHG5i"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuCiLLRup-Ih"
      },
      "source": [
        "model, encoder_inputs, encoder_outputs1, final_enc_h, final_enc_c, dec_emb_layer, decoder_inputs, \\\n",
        "decoder_dense, decoder_lstm, attention_layer = get_model(source_VOCAB_SIZE, max_source_len, target_VOCAB_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7VuylsjrZIg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f94ec180-004d-4c25-adeb-f8bd3f1bd7be"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 72)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 72, 1024)     25254912    ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  [(None, 72, 512),    2623488     ['embedding[0][0]']              \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 1024)   45181952    ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 512)          0           ['bidirectional[0][1]',          \n",
            "                                                                  'bidirectional[0][3]']          \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 512)          0           ['bidirectional[0][2]',          \n",
            "                                                                  'bidirectional[0][4]']          \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 512),  3147776     ['embedding_1[0][0]',            \n",
            "                                 (None, 512),                     'concatenate[0][0]',            \n",
            "                                 (None, 512)]                     'concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " attention_layer (AttentionLaye  ((None, None, 512),  524800     ['bidirectional[0][0]',          \n",
            " r)                              (None, None, 72))                'lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            " concat_layer (Concatenate)     (None, None, 1024)   0           ['lstm_1[0][0]',                 \n",
            "                                                                  'attention_layer[0][0]']        \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 44123)  45226075    ['concat_layer[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 121,959,003\n",
            "Trainable params: 121,959,003\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rULqsRlPrgvB"
      },
      "source": [
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sswj7-xorhw0"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "filepath = \"weights-improvement-{loss:.4f}-bigger.hdf5\" \n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath, monitor='loss', \n",
        "    verbose=0,        \n",
        "    save_best_only=False,\n",
        "    save_frec=\"epoch\"        \n",
        ")  \n",
        "#checkpoint = ModelCheckpoint(\"model3/\", monitor='val_accuracy')\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=2)\n",
        "callbacks_list = [checkpoint, early_stopping]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwq7N5WYrkxh"
      },
      "source": [
        "# Training\n",
        "encoder_input_data = X_train\n",
        "# To make same as target data skip last number which is just padding\n",
        "decoder_input_data = y_train[:,:-1]\n",
        "# Decoder target data has to be one step ahead so we are taking from 1 as told in keras docs\n",
        "decoder_target_data =  y_train[:,1:]\n",
        "\n",
        "# Testing\n",
        "encoder_input_test = X_test\n",
        "decoder_input_test = y_test[:,:-1]\n",
        "decoder_target_test=  y_test[:,1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feS57pREu5_X"
      },
      "source": [
        "En la siguiente linea de código entrenamos nuestro modelo, con pocas epocas ya que para cada una de ella si tarda mucho."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8u68O4Nrk2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb264937-e3e7-4ba1-da7a-84e84948cfbd"
      },
      "source": [
        "EPOCHS= 10\n",
        "history = model.fit([encoder_input_data, decoder_input_data],decoder_target_data, \n",
        "                    epochs=EPOCHS, \n",
        "                    batch_size=128,\n",
        "                    validation_data = ([encoder_input_test, decoder_input_test],decoder_target_test),\n",
        "                    callbacks= callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "948/948 [==============================] - 799s 835ms/step - loss: 0.6169 - accuracy: 0.9220 - val_loss: 0.4433 - val_accuracy: 0.9345\n",
            "Epoch 2/10\n",
            "948/948 [==============================] - 791s 834ms/step - loss: 0.3402 - accuracy: 0.9469 - val_loss: 0.2848 - val_accuracy: 0.9554\n",
            "Epoch 3/10\n",
            "948/948 [==============================] - 791s 834ms/step - loss: 0.2079 - accuracy: 0.9627 - val_loss: 0.2257 - val_accuracy: 0.9635\n",
            "Epoch 4/10\n",
            "948/948 [==============================] - 792s 835ms/step - loss: 0.1348 - accuracy: 0.9715 - val_loss: 0.2075 - val_accuracy: 0.9658\n",
            "Epoch 5/10\n",
            "948/948 [==============================] - 791s 835ms/step - loss: 0.0915 - accuracy: 0.9784 - val_loss: 0.2020 - val_accuracy: 0.9674\n",
            "Epoch 6/10\n",
            "948/948 [==============================] - 791s 835ms/step - loss: 0.0673 - accuracy: 0.9832 - val_loss: 0.2045 - val_accuracy: 0.9674\n",
            "Epoch 7/10\n",
            "948/948 [==============================] - 791s 834ms/step - loss: 0.0534 - accuracy: 0.9862 - val_loss: 0.2067 - val_accuracy: 0.9678\n",
            "Epoch 8/10\n",
            "948/948 [==============================] - 791s 835ms/step - loss: 0.0442 - accuracy: 0.9883 - val_loss: 0.2105 - val_accuracy: 0.9677\n",
            "Epoch 9/10\n",
            "948/948 [==============================] - 791s 834ms/step - loss: 0.0373 - accuracy: 0.9900 - val_loss: 0.2151 - val_accuracy: 0.9678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuEHBr-eatWe"
      },
      "source": [
        "encoder_model, decoder_model = \\\n",
        "                                get_inference_model(encoder_inputs, encoder_outputs1, final_enc_h, \\\n",
        "                                final_enc_c, dec_emb_layer, decoder_inputs, decoder_dense, \\\n",
        "                               decoder_lstm, attention_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YOeO2W6gC0j"
      },
      "source": [
        "### b) Implementa una función `plot_attention(source_txt, target_txt, attention)` para visualizar los pesos de atención entre una sentencia del lenguaje fuente y su respectiva traducción, lo que permite visualizar también su \"alineamiento\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqEIU6qYq8tw"
      },
      "source": [
        "encoder_model, decoder_model = \\\n",
        "                                get_inference_model(encoder_inputs, encoder_outputs1, final_enc_h, \\\n",
        "                                final_enc_c, dec_emb_layer, decoder_inputs, decoder_dense, \\\n",
        "                               decoder_lstm, attention_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeuVhcBcw933"
      },
      "source": [
        "def get_predicted_sentence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    enc_output, enc_h, enc_c = encoder_model.predict(input_seq)\n",
        "    # attention weights for each token in output sequence\n",
        "    attention_weights = np.empty(shape=[0, max_source_len])\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    \n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_word_index['<start>']\n",
        "    \n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = '<start>'\n",
        "    \n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c, attention_w, attention_c = decoder_model.predict([target_seq] + [enc_output, enc_h, enc_c ])\n",
        "        #print(attention_w.shape)\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        if sampled_token_index == 0:\n",
        "          break\n",
        "        else:\n",
        "            # convert max index number to spanish word\n",
        "            sampled_char = target_index_word[sampled_token_index]\n",
        "\n",
        "        if (sampled_char!='<end>'):\n",
        "            # aapend it ti decoded sent\n",
        "            decoded_sentence += ' '+sampled_char\n",
        "            attention_weights = np.append(attention_weights,[attention_w.reshape(-1,)],axis=0)\n",
        "        \n",
        "        # Exit condition: either hit max length or find stop token.\n",
        "        if (sampled_char == '<end>' or len(decoded_sentence.split()) >= max_source_len):\n",
        "            attention_weights = np.append(attention_weights,[attention_w.reshape(-1,)],axis=0)\n",
        "            decoded_sentence += ' '+'<end>'\n",
        "            stop_condition = True\n",
        "        \n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        \n",
        "        # Update states\n",
        "        enc_h, enc_c = h, c\n",
        "    \n",
        "    return decoded_sentence, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arGmoc0SEo3J"
      },
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    \n",
        "    num_digits= str.maketrans('','', digits)\n",
        "    sentence= sentence.lower()\n",
        "    sentence= re.sub(\" +\", \" \", sentence)\n",
        "    sentence= re.sub(\" \", \" \", sentence)\n",
        "    #sentence= re.sub(\"'\", '', sentence)\n",
        "    sentence= sentence.translate(num_digits)\n",
        "    #sentence= re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
        "    sentence = sentence.rstrip().strip()\n",
        "    sentence=  '<start> ' + sentence + '\\t<end> '\n",
        "    sentence = \" \".join(sentence.split())\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqMOFZPD1NaD"
      },
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax= fig.add_subplot(1,1,1)\n",
        "    ax.matshow(attention)\n",
        "    fontdict={'fontsize':10}\n",
        "    \n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence[1::], fontdict=fontdict)\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiSWuc-vvWQx"
      },
      "source": [
        "### c) Implementa una función `translate_text(source_txt)` que pueda realizar la traducción de sentencias nuevas, es decir, que no estén en el corpus que usaste para entrenar o validar tu modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj_4DxI-uDTN"
      },
      "source": [
        "def translate(sentence):\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "  \n",
        "  #convert the sentence to index based on word2index dictionary\n",
        "  inputs= [source_word_index[i] for i in sentence.split(' ')]\n",
        "  # pad the sequence \n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_source_len, padding='post')\n",
        "\n",
        "  #conver to tensors\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  trad, attention =get_predicted_sentence(inputs)\n",
        "  print('Input : %s' % (sentence))\n",
        "  print('predicted sentence :{}'.format(trad))\n",
        "\n",
        "  #attention_plot = np.zeros((max_target_len, max_source_len))\n",
        "  \n",
        "  attention_plot= attention[:len(trad.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), trad.split(' '))\n",
        "\n",
        "  return(sentence,trad,attention,inputs )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "WQVQG5WF-XB5",
        "outputId": "20416d9f-7cd3-43be-f78f-abee6b188332"
      },
      "source": [
        "sentence,trad,attention, inputs_ =translate(\"Hello World.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : <start> hello world. <end>\n",
            "predicted sentence :<start> ¡hola a todo el mundo. <end>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZS7ll9az9pc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "outputId": "6f63214f-8bd5-401b-a9da-383599d7848d"
      },
      "source": [
        "sentence,trad,attention, inputs_ =translate(\"All we need is love.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : <start> all we need is love. <end>\n",
            "predicted sentence :<start> lo que necesitamos es amor. <end>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1xBsqmFgC0k"
      },
      "source": [
        "En nuestros dos ejemplos mostramos resultados de haber realizado la traducción en ambos observamos que la traducción no es mala por lo que podemos decir que los resultados son buenos al traducir de inglés a español."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MKcHZ20gC0k"
      },
      "source": [
        "### d) Una métrica para evaluar la calidad de la traducción es BLEU (Bilingual Evaluation Understudy, Papinoni et al., 2002). Realiza la evaluación de tu modelo mediante BLEU usando un conjunto de sentencias de prueba. Da tu opinión sobre la calidad de la traducción. Puedes apoyarte en https://blog.machinetranslation.io/compute-bleu-score/ para la implementación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olwwwn6i6Xz9"
      },
      "source": [
        "def get_target_sentence(input_sequence):\n",
        "    sentence =''\n",
        "    for i in input_sequence:\n",
        "      if i!=0 :\n",
        "        sentence =sentence +target_index_word[i]+' '\n",
        "    return sentence \n",
        "\n",
        "def get_source_sentence(input_sequence):\n",
        "    sentence =''\n",
        "    for i in input_sequence:\n",
        "      if i!=0:\n",
        "        sentence =sentence +source_index_word[i]+' '\n",
        "    return sentence  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57tSa3ff0N6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd58d1a2-b678-4368-b91e-518d494a5feb"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13474, 72)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS3vRhW42MTg"
      },
      "source": [
        "target_=[]\n",
        "predict_=[]\n",
        "for i in np.random.randint(1, 13473, size=1000):\n",
        "    target_.append(get_target_sentence(y_test[i]))\n",
        "    sent, attention = get_predicted_sentence(X_test[i].reshape(1,max_source_len))\n",
        "    predict_.append(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG4lwyJJFuqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c7497f6-7f38-4d03-a53d-832fe4958679"
      },
      "source": [
        "!pip install sacrebleu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▋                            | 10 kB 32.6 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 20 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 30 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 40 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 51 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 61 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 71 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 81 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 90 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.9)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (1.19.5)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.4 portalocker-2.3.2 sacrebleu-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl4De6A34N8Q",
        "outputId": "7e3f1c4c-6c53-41e4-ee69-d510524c64d1"
      },
      "source": [
        "!pip install sacremoses"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 61 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 71 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 81 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 92 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 102 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 112 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 122 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 133 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 143 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 153 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 163 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 174 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 184 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 194 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 204 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 215 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 225 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 235 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 245 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 256 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 266 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 276 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 286 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 296 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 307 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 317 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 327 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 337 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 348 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 358 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 368 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 378 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 389 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 399 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 409 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 419 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 430 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 440 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 450 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 460 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 471 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 481 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 491 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 501 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 512 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 522 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 532 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 542 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 552 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 563 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 573 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 583 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 593 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 604 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 614 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 624 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 634 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 645 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 655 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 665 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 675 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 686 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 696 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 706 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 716 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 727 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 737 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 747 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 757 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 768 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 778 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 788 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 798 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 808 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 819 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 829 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 839 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 849 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 860 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 870 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 880 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 890 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 895 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.62.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypYWaKqo3nSZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ef4c0ed-110b-4db7-c620-f5b300059d8e"
      },
      "source": [
        "import sacrebleu\n",
        "from sacremoses import MosesDetokenizer\n",
        "md = MosesDetokenizer(lang='en')\n",
        "\n",
        "\n",
        "# Open the test dataset human translation file and detokenize the references\n",
        "refs = []\n",
        "\n",
        "for line in target_: \n",
        "  line = line.strip().split() \n",
        "  line = md.detokenize(line) \n",
        "  refs.append(line)\n",
        "    \n",
        "print(\"Reference 1st sentence:\", refs[0])\n",
        "\n",
        "refs = [refs]  # Yes, it is a list of list(s) as required by sacreBLEU\n",
        "\n",
        "\n",
        "# Open the translation file by the NMT model and detokenize the predictions\n",
        "preds = []\n",
        "\n",
        "for line in predict_: \n",
        "  line = line.strip().split() \n",
        "  line = md.detokenize(line) \n",
        "  preds.append(line)\n",
        "\n",
        "print(\"MTed 1st sentence:\", preds[0])    \n",
        "\n",
        "\n",
        "# Calculate and print the BLEU score\n",
        "bleu = sacrebleu.corpus_bleu(preds, refs)\n",
        "print(bleu.score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference 1st sentence: <start> se acabó todo. <end>\n",
            "MTed 1st sentence: <start> todo está hecho. <end>\n",
            "61.22086229545785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUufAQVzwa2K"
      },
      "source": [
        "En la anterior linea encontramos los resultados de evaluar la calidad de la metrica, nos da un resultado bueno."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NKIA4xH-hQ_"
      },
      "source": [
        "# Español - Ingles\n",
        "Ahora lo que vamos a realizar es basicamente lo mismo que se hizo para el modelo inglés-español pero ahora buscamos un modelo de español a inglés."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2MCBywd6zoo"
      },
      "source": [
        "# convert to list for tokenizer\n",
        "source_texts = df.Spanish.to_list() \n",
        "target_texts = df.English.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SKfpeGD6zvU",
        "outputId": "eb32bfd2-c1e8-4fcb-d77b-cc4d55a747c4"
      },
      "source": [
        "def tokenize_sent(text):\n",
        "  '''\n",
        "  Take list on texts as input and \n",
        "  returns its tokenizer and enocded text\n",
        "  '''\n",
        "  tokenizer = Tokenizer(filters = '')\n",
        "  tokenizer.fit_on_texts(text)\n",
        "\n",
        "  return tokenizer, tokenizer.texts_to_sequences(text)\n",
        "\n",
        "# Tokenize source and target sentences\n",
        "source_tokenizer, source_encoded= tokenize_sent(text= source_texts)\n",
        "target_tokenizer, target_encoded= tokenize_sent(text= target_texts)\n",
        "\n",
        "# diccionarios para el mapeo entre one-hot encodings y vocabulario\n",
        "source_index_word = source_tokenizer.index_word\n",
        "source_word_index= source_tokenizer.word_index\n",
        "source_VOCAB_SIZE = len(source_tokenizer.word_counts)+1\n",
        "target_index_word = target_tokenizer.index_word\n",
        "target_word_index= target_tokenizer.word_index\n",
        "target_VOCAB_SIZE=len(target_tokenizer.word_counts)+1\n",
        "\n",
        "max_source_len = 0\n",
        "for i in range(len(source_encoded)):\n",
        "  if len(source_encoded[i]) > max_source_len:\n",
        "    max_source_len= len(source_encoded[i])\n",
        "\n",
        "max_target_len = 0\n",
        "for i in range(len(target_encoded)):\n",
        "  if len(source_encoded[i]) > max_target_len:\n",
        "    max_target_len= len(target_encoded[i])\n",
        "\n",
        "print('Source language')\n",
        "print('Vocabulary size: ', source_VOCAB_SIZE, 'Max sequence length: ', max_source_len)\n",
        "print('Target language')\n",
        "print('Vocabulary size: ', target_VOCAB_SIZE, 'Max sequence length: ', max_target_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source language\n",
            "Vocabulary size:  44123 Max sequence length:  70\n",
            "Target language\n",
            "Vocabulary size:  24663 Max sequence length:  72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-SoMJG76zyB"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "source_padded = pad_sequences(source_encoded, maxlen=max_source_len, padding='post')\n",
        "target_padded = pad_sequences(target_encoded, maxlen=max_target_len, padding='post')\n",
        "# converting to array for processing\n",
        "source_padded= np.array(source_padded)\n",
        "target_padded= np.array(target_padded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgnPP0h86z1B",
        "outputId": "a786ea6c-15e2-431b-f3d3-abc2173958ad"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(source_padded, target_padded, test_size=0.1, random_state=0)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((121262, 70), (13474, 70), (121262, 72), (13474, 72))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B32-WWghD8BN"
      },
      "source": [
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W_a_dot_s.shape)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>',U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
        "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
        "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
        "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
        "            return fake_state\n",
        "\n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK5-xYIRDTr8"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Embedding, Concatenate, Dropout\n",
        "from tensorflow.keras import Input, Model\n",
        "#from BahdanauAttention import AttentionLayer\n",
        "from keras.layers.recurrent import LSTMCell\n",
        "\n",
        "def get_encoder(vocab_size, source_len):\n",
        "    encoder_inputs = Input(shape=(source_len,)) \n",
        "    enc_emb = Embedding(vocab_size, 1024)(encoder_inputs)\n",
        "    # Bidirectional lstm layer\n",
        "    enc_lstm1 = Bidirectional(LSTM(256,return_sequences=True,return_state=True))\n",
        "    encoder_outputs_lstm1, forw_state_h, forw_state_c, back_state_h, back_state_c = enc_lstm1(enc_emb)\n",
        "    final_enc_h = Concatenate()([forw_state_h,back_state_h])\n",
        "    final_enc_c = Concatenate()([forw_state_c,back_state_c])\n",
        "    # concatenate encoder states\n",
        "    encoder_states =[final_enc_h, final_enc_c]\n",
        "    \n",
        "    return encoder_inputs, encoder_outputs_lstm1, final_enc_h, final_enc_c\n",
        "\n",
        "def get_decoder(vocab_size, encoder_states):\n",
        "    decoder_inputs = Input(shape=(None,)) \n",
        "    dec_emb_layer = Embedding(vocab_size, 1024) \n",
        "    dec_emb = dec_emb_layer(decoder_inputs)\n",
        "    #LSTM using encoder_states as initial state\n",
        "    decoder_lstm = LSTM(512, return_sequences=True, return_state=True) \n",
        "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "    \n",
        "    return(decoder_inputs, decoder_outputs, dec_emb_layer, decoder_lstm)\n",
        "\n",
        "def get_model(source_vocabsize, source_maxlen, target_vocabsize):\n",
        "    # encoder\n",
        "    encoder_inputs, encoder_outputs1, final_enc_h, final_enc_c = get_encoder(source_vocabsize, source_maxlen)\n",
        "    # concatenate encoder states\n",
        "    encoder_states =[final_enc_h, final_enc_c]\n",
        "    # decoder\n",
        "    decoder_inputs, decoder_outputs, dec_emb_layer, decoder_lstm = get_decoder(target_vocabsize, encoder_states)\n",
        "    #Attention Layer\n",
        "    attention_layer = AttentionLayer()\n",
        "    attention_result, attention_weights = attention_layer([encoder_outputs1, decoder_outputs])\n",
        "    # Concat attention output and decoder LSTM output \n",
        "    decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attention_result])\n",
        "    #Dense layer\n",
        "    decoder_dense = Dense(target_vocabsize, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "    # Define the model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
        "    \n",
        "    return model, encoder_inputs, encoder_outputs1, final_enc_h, final_enc_c, dec_emb_layer, \\\n",
        "            decoder_inputs, decoder_dense, decoder_lstm, attention_layer\n",
        "\n",
        "def get_inference_model(encoder_inputs, encoder_outputs1, final_enc_h, final_enc_c, dec_emb_layer, \\\n",
        "                        decoder_inputs, decoder_dense, decoder_lstm, attention_layer):\n",
        "    encoder_model = Model(encoder_inputs, outputs = [encoder_outputs1, final_enc_h, final_enc_c])\n",
        "    decoder_state_h = Input(shape=(512,))\n",
        "    decoder_state_c = Input(shape=(512,))\n",
        "    decoder_hidden_state_input = Input(shape=(max_source_len,512))\n",
        "    dec_states = [decoder_state_h, decoder_state_c]\n",
        "    dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "    decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=dec_states)\n",
        "\n",
        "    # Attention inference\n",
        "    attention_result_inf, attention_weights_inf = attention_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "    decoder_concat_input_inf = Concatenate(axis=-1, name='concat_layer')([decoder_outputs2, attention_result_inf])\n",
        "\n",
        "    dec_states2= [state_h2, state_c2]\n",
        "    decoder_outputs2 = decoder_dense(decoder_concat_input_inf)\n",
        "\n",
        "    decoder_model= Model(\n",
        "                        [decoder_inputs] + [decoder_hidden_state_input, decoder_state_h, decoder_state_c],\n",
        "                         [decoder_outputs2]+ dec_states2 + [attention_weights_inf] + [attention_result_inf])\n",
        "    \n",
        "    return encoder_model, decoder_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bIKhRX_DTu3"
      },
      "source": [
        "model, encoder_inputs, encoder_outputs1, final_enc_h, final_enc_c, dec_emb_layer, decoder_inputs, \\\n",
        "decoder_dense, decoder_lstm, attention_layer = get_model(source_VOCAB_SIZE, max_source_len, target_VOCAB_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLmHoNSHDT1a",
        "outputId": "ce3d9b5e-9db6-4bc4-e5b1-757a5595c580"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_9 (InputLayer)           [(None, 70)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 70, 1024)     45181952    ['input_9[0][0]']                \n",
            "                                                                                                  \n",
            " input_10 (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirectional  [(None, 70, 512),   2623488     ['embedding_2[0][0]']            \n",
            " )                               (None, 256),                                                     \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, None, 1024)   25254912    ['input_10[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 512)          0           ['bidirectional_1[0][1]',        \n",
            "                                                                  'bidirectional_1[0][3]']        \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 512)          0           ['bidirectional_1[0][2]',        \n",
            "                                                                  'bidirectional_1[0][4]']        \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 512),  3147776     ['embedding_3[0][0]',            \n",
            "                                 (None, 512),                     'concatenate_2[0][0]',          \n",
            "                                 (None, 512)]                     'concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " attention_layer_1 (AttentionLa  ((None, None, 512),  524800     ['bidirectional_1[0][0]',        \n",
            " yer)                            (None, None, 70))                'lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            " concat_layer (Concatenate)     (None, None, 1024)   0           ['lstm_3[0][0]',                 \n",
            "                                                                  'attention_layer_1[0][0]']      \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, None, 24663)  25279575    ['concat_layer[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 102,012,503\n",
            "Trainable params: 102,012,503\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veGyCdJJEBTM"
      },
      "source": [
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5rxiyznEBXb"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "filepath = \"weights-improvement-{loss:.4f}-bigger.hdf5\" \n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath, monitor='loss', \n",
        "    verbose=0,        \n",
        "    save_best_only=False,\n",
        "    save_frec=\"epoch\"        \n",
        ")  \n",
        "#checkpoint = ModelCheckpoint(\"model3/\", monitor='val_accuracy')\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=2)\n",
        "callbacks_list = [checkpoint, early_stopping]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpl989bdEBbT"
      },
      "source": [
        "# Training\n",
        "encoder_input_data = X_train\n",
        "# To make same as target data skip last number which is just padding\n",
        "decoder_input_data = y_train[:,:-1]\n",
        "# Decoder target data has to be one step ahead so we are taking from 1 as told in keras docs\n",
        "decoder_target_data =  y_train[:,1:]\n",
        "\n",
        "# Testing\n",
        "encoder_input_test = X_test\n",
        "decoder_input_test = y_test[:,:-1]\n",
        "decoder_target_test=  y_test[:,1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96pjKqzqEIS2",
        "outputId": "045b2009-bbb3-4ac0-d315-79035aa57e84"
      },
      "source": [
        "EPOCHS= 10\n",
        "history = model.fit([encoder_input_data, decoder_input_data],decoder_target_data, \n",
        "                    epochs=EPOCHS, \n",
        "                    batch_size=128,\n",
        "                    validation_data = ([encoder_input_test, decoder_input_test],decoder_target_test),\n",
        "                    callbacks= callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "948/948 [==============================] - 603s 632ms/step - loss: 0.5527 - accuracy: 0.9244 - val_loss: 0.3800 - val_accuracy: 0.9397\n",
            "Epoch 2/10\n",
            "948/948 [==============================] - 597s 630ms/step - loss: 0.2877 - accuracy: 0.9526 - val_loss: 0.2336 - val_accuracy: 0.9614\n",
            "Epoch 3/10\n",
            "948/948 [==============================] - 597s 630ms/step - loss: 0.1688 - accuracy: 0.9688 - val_loss: 0.1814 - val_accuracy: 0.9688\n",
            "Epoch 4/10\n",
            "948/948 [==============================] - 598s 630ms/step - loss: 0.1069 - accuracy: 0.9773 - val_loss: 0.1624 - val_accuracy: 0.9719\n",
            "Epoch 5/10\n",
            "948/948 [==============================] - 598s 631ms/step - loss: 0.0705 - accuracy: 0.9835 - val_loss: 0.1580 - val_accuracy: 0.9729\n",
            "Epoch 6/10\n",
            "948/948 [==============================] - 598s 631ms/step - loss: 0.0496 - accuracy: 0.9877 - val_loss: 0.1578 - val_accuracy: 0.9734\n",
            "Epoch 7/10\n",
            "948/948 [==============================] - 598s 631ms/step - loss: 0.0373 - accuracy: 0.9904 - val_loss: 0.1590 - val_accuracy: 0.9737\n",
            "Epoch 8/10\n",
            "948/948 [==============================] - 598s 631ms/step - loss: 0.0290 - accuracy: 0.9923 - val_loss: 0.1617 - val_accuracy: 0.9738\n",
            "Epoch 9/10\n",
            "948/948 [==============================] - 598s 631ms/step - loss: 0.0232 - accuracy: 0.9938 - val_loss: 0.1660 - val_accuracy: 0.9739\n",
            "Epoch 10/10\n",
            "948/948 [==============================] - 599s 632ms/step - loss: 0.0192 - accuracy: 0.9948 - val_loss: 0.1698 - val_accuracy: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cShXdP0EIVv"
      },
      "source": [
        "encoder_model, decoder_model = \\\n",
        "                                get_inference_model(encoder_inputs, encoder_outputs1, final_enc_h, \\\n",
        "                                final_enc_c, dec_emb_layer, decoder_inputs, decoder_dense, \\\n",
        "                               decoder_lstm, attention_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DhYdTe0EIYS"
      },
      "source": [
        "def get_predicted_sentence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    enc_output, enc_h, enc_c = encoder_model.predict(input_seq)\n",
        "    # attention weights for each token in output sequence\n",
        "    attention_weights = np.empty(shape=[0, max_source_len])\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    \n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_word_index['<start>']\n",
        "    \n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = '<start>'\n",
        "    \n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c, attention_w, attention_c = decoder_model.predict([target_seq] + [enc_output, enc_h, enc_c ])\n",
        "        #print(attention_w.shape)\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        if sampled_token_index == 0:\n",
        "          break\n",
        "        else:\n",
        "            # convert max index number to spanish word\n",
        "            sampled_char = target_index_word[sampled_token_index]\n",
        "\n",
        "        if (sampled_char!='<end>'):\n",
        "            # aapend it ti decoded sent\n",
        "            decoded_sentence += ' '+sampled_char\n",
        "            attention_weights = np.append(attention_weights,[attention_w.reshape(-1,)],axis=0)\n",
        "        \n",
        "        # Exit condition: either hit max length or find stop token.\n",
        "        if (sampled_char == '<end>' or len(decoded_sentence.split()) >= max_source_len):\n",
        "            attention_weights = np.append(attention_weights,[attention_w.reshape(-1,)],axis=0)\n",
        "            decoded_sentence += ' '+'<end>'\n",
        "            stop_condition = True\n",
        "        \n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        \n",
        "        # Update states\n",
        "        enc_h, enc_c = h, c\n",
        "    \n",
        "    return decoded_sentence, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-muhgJkDT4J"
      },
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    \n",
        "    num_digits= str.maketrans('','', digits)\n",
        "    sentence= sentence.lower()\n",
        "    sentence= re.sub(\" +\", \" \", sentence)\n",
        "    sentence= re.sub(\" \", \" \", sentence)\n",
        "    #sentence= re.sub(\"'\", '', sentence)\n",
        "    sentence= sentence.translate(num_digits)\n",
        "    #sentence= re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
        "    sentence = sentence.rstrip().strip()\n",
        "    sentence=  '<start> ' + sentence + '\\t<end> '\n",
        "    sentence = \" \".join(sentence.split())\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xccLSHQDT9t"
      },
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax= fig.add_subplot(1,1,1)\n",
        "    ax.matshow(attention)\n",
        "    fontdict={'fontsize':10}\n",
        "    \n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence[1::], fontdict=fontdict)\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH4zswIdE8Qb"
      },
      "source": [
        "def translate(sentence):\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "  \n",
        "  #convert the sentence to index based on word2index dictionary\n",
        "  inputs= [source_word_index[i] for i in sentence.split(' ')]\n",
        "  # pad the sequence \n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_source_len, padding='post')\n",
        "\n",
        "  #conver to tensors\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  trad, attention =get_predicted_sentence(inputs)\n",
        "  print('Input : %s' % (sentence))\n",
        "  print('predicted sentence :{}'.format(trad))\n",
        "\n",
        "  #attention_plot = np.zeros((max_target_len, max_source_len))\n",
        "  \n",
        "  attention_plot= attention[:len(trad.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), trad.split(' '))\n",
        "\n",
        "  return(sentence,trad,attention,inputs )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "giqtSfakE8S9",
        "outputId": "f89caec1-68b8-430f-9b7a-000faf17a4cd"
      },
      "source": [
        "sentence,trad,attention, inputs_ =translate(\"Hola mundo.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : <start> hola mundo. <end>\n",
            "predicted sentence :<start> hi, guys. <end>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "uThlY0mvE8XR",
        "outputId": "4a321bfa-0792-4114-c2c7-cbcc78c42052"
      },
      "source": [
        "sentence,trad,attention, inputs_ =translate(\"Somos los martillos y las ruedas.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : <start> somos los martillos y las ruedas <end>\n",
            "predicted sentence :<start> we're the subject and the guys. <end>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "LOB4VVq9DUBV",
        "outputId": "02387987-9ae9-4986-98c9-9172d0679c29"
      },
      "source": [
        "sentence,trad,attention, inputs_ =translate(\"Hoy viernes me di cuenta que la noche no es tan larga.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : <start> hoy viernes me di cuenta que la noche no es tan larga. <end>\n",
            "predicted sentence :<start> today i was today when i isn't so long in the long time. <end>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXUKCMg0xGHZ"
      },
      "source": [
        "En este caso dimos tres ejemplos, en ellos sobservamos que los resultados no fueron tan malos, en algunos de ellos cambiaron varias palabras por otras que se pueden considerar como equivocadas. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsPWyQUrgC0l"
      },
      "source": [
        "e) El modelo NMT que usaste utiliza una capa de atención basado en Bahdanau et. al., (2015), pero otras opciones son las llamadas \"Atención Global\" y \"Atención Local\" propuesta por Luong et. al., (2015). Describe brevemente las diferencias entre éstos enfoques de atención, y cuál crees que es mejor para la tarea de traducción automática.\n",
        "\n",
        "OPCIONAL: incluye los modelos de atención de Luong y verifica su desempeño en tu modelo NMT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt-47er6yCCG"
      },
      "source": [
        "##### Atención Global\n",
        "\n",
        "La atención global es considerar todos los estados ocultos de codificador al derivar ek vector de contexto $c_t$, en este modelo, un vector llamado de longitud variables, cuyo tamaño es igual al número de pasos de tiempo en el lado de la fuente, se deriva comprando el estado oculto de destino actual $h_t$ con cada estado oculto de origen. En cada pas de tiempo  $t$, el modelo infiere un vector de peso de alineación de longitud variable $a_t$ basado en el estado actual $h_t$ y todos los estados fuente.\n",
        "\n",
        "##### Atención Local\n",
        "\n",
        "La atención local surgio como una alternativa para atender algunas deficiencias de la atención global. En ella se tiene que prestar atención a todas las palabras en el lado del corpus fuente para cada palabra de destino, pero es poco practico y efectivo traducir sentencias más largas como párrafos o documentos. Por lo que se propusó un mecanismo de tención local que elige centrarse sólo en un pequeño subconjunto de las posisiciones de origen por palabra de destino."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJp9kHhLgC0l"
      },
      "source": [
        "### f) OPCIONAL. Un problema en particular con el inglés, son las contracciones. Por ejemplo, en el modelo que yo entrené tengo éstos resultados.\n",
        "\n",
        "Texto:  `<start> i'm young and a good guy <end>`\n",
        "\n",
        "Traducción:  `<start> soy joven y un buen tipo. <end>`\n",
        "\n",
        "Texto:  `<start> i am young and a good guy <end>`\n",
        "\n",
        "Traducción:  `<start> soy joven como un buen tipo. <end>`\n",
        "\n",
        "Una forma de resolverlo es incluir ambos tipos de versiones (contracción y completa) relacionadas con su respectiva traducción en el corpus de entrenamiento. Hay varias formas de hacerlo. Por ejemplo, en el código que se muestra en la siguiente celda se hace realiza la expansión el texto basado en un diccionario de contracciones de inglés (contraction_expansion.txt). Implementa ésta idea para aumentar el corpus de entrenamiento y verifica si el desempeño del traductor  mejora."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWYkr2g0gC0m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "229ed9f1-5d86-4da1-cb25-1c0b43c7623e"
      },
      "source": [
        "with open(\"contraction_expansion.txt\", 'rb') as fp:\n",
        "    contractions= pickle.load(fp)\n",
        "\n",
        "def expand_eng_text(text):\n",
        "    if type(text) is str:\n",
        "        for key in contractions:\n",
        "            value = contractions[key]\n",
        "            text = text.replace(key, value)\n",
        "        return text\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# ejemplo:\n",
        "txt = \"i'm don't he'll you'll\"\n",
        "print(expand_contras(txt))\n",
        ">> 'i am do not he will you will'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-59-3dea09912257>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    >> 'i am do not he will you will'\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_fxkBkVgC0m"
      },
      "source": [
        "# TU CÓDIGO AQUI"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}